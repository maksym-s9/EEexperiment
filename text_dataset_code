import time
import numpy as np
import pandas as pd
import tensorflow as tf
from sklearn.model_selection import train_test_split
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Embedding, LSTM, SpatialDropout1D
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras import backend as K

# Load dataset (update path if needed)
df = pd.read_csv("IMDB Dataset.csv")

# Preprocess the data
texts = df['review'].values  # Update the column name to match your dataset
labels = (df['sentiment'] == 'positive').astype(int).values  # 1 for positive, 0 for negative

# Tokenize the text data
tokenizer = Tokenizer(num_words=5000, lower=True, split=' ')
tokenizer.fit_on_texts(texts)
X = tokenizer.texts_to_sequences(texts)
X = pad_sequences(X, maxlen=200)

# Split into training and validation sets
X_train, X_val, y_train, y_val = train_test_split(X, labels, test_size=0.2, random_state=42)

# Build the model
model = Sequential()
model.add(Embedding(input_dim=5000, output_dim=128, input_length=200))
model.add(SpatialDropout1D(0.2))
model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))
model.add(Dense(1, activation='sigmoid'))

model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

# Custom training loop
def custom_train_loop(model, X_train, y_train, X_val, y_val, epochs=10, batch_size=64, threshold=0.75):
    start_time = time.time()
    total_batches = len(X_train) // batch_size
    for epoch in range(epochs):
        print(f"\nEpoch {epoch+1}/{epochs}")
        epoch_start_time = time.time()
        
        # Shuffle the training data for each epoch
        indices = np.random.permutation(len(X_train))
        X_train_shuffled = X_train[indices]
        y_train_shuffled = y_train[indices]
        
        for batch in range(total_batches):
            batch_start = batch * batch_size
            batch_end = (batch + 1) * batch_size
            X_batch = X_train_shuffled[batch_start:batch_end]
            y_batch = y_train_shuffled[batch_start:batch_end]
            
            # Train on the batch
            model.train_on_batch(X_batch, y_batch)
            
            # Compute the validation accuracy after each batch
            val_loss, val_acc = model.evaluate(X_val, y_val, verbose=0)
            
            # Check if validation accuracy has reached the threshold
            if val_acc >= threshold:
                elapsed_time = time.time() - start_time
                print(f"Validation accuracy reached {threshold * 100}% at batch {batch + 1} in epoch {epoch + 1} in {elapsed_time:.2f} seconds.")
                return

            # Print progress every 50 batches
            if batch % 50 == 0:
                print(f"Batch {batch+1}/{total_batches} - val_accuracy: {val_acc:.4f}")
        
        epoch_time = time.time() - epoch_start_time
        print(f"Epoch {epoch+1} completed in {epoch_time:.2f} seconds.")
        
    print("Training completed.")

# Train the model using custom loop
custom_train_loop(model, X_train, y_train, X_val, y_val, epochs=10, batch_size=64, threshold=0.75)
